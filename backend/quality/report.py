"""Generate Markdown quality report from evaluation scores."""
from datetime import datetime


def generate_quality_report_md(scores: dict, output_path: str) -> None:
    """Write a Markdown evaluation report to disk from the scores dict.

    Args:
        scores: Output of `run_quality_check()` — must contain faithfulness,
                answer_relevancy, context_recall, and per_question keys.
        output_path: Absolute path to the output file (e.g. "reports/eval.md").
    """
    faithfulness = scores.get("faithfulness", 0.0)
    answer_relevancy = scores.get("answer_relevancy", 0.0)
    context_recall = scores.get("context_recall", 0.0)
    per_question = scores.get("per_question", [])
    avg = (faithfulness + answer_relevancy + context_recall) / 3

    lines = [
        "# PALO RAG — Quality Evaluation Report",
        "",
        f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**Dataset**: {len(per_question)} reference questions",
        "",
        "## Summary",
        "",
        "| Metric | Score |",
        "|--------|-------|",
        f"| Faithfulness | {faithfulness:.2f} |",
        f"| Answer Relevancy | {answer_relevancy:.2f} |",
        f"| Context Recall | {context_recall:.2f} |",
        f"| **Average** | **{avg:.2f}** |",
        "",
        "## Per-Question Results",
        "",
        "| # | Question | Source Found | Answer Length |",
        "|---|----------|-------------|---------------|",
    ]

    for i, item in enumerate(per_question, 1):
        q = item.get("question", "")[:60].replace("|", "\\|")
        source_found = "✓" if item.get("source_found") else "✗"
        answer_len = item.get("answer_length", 0)
        lines.append(f"| {i} | {q} | {source_found} | {answer_len} |")

    lines += [
        "",
        "---",
        "*Report auto-generated by PALO RAG quality runner.*",
    ]

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines) + "\n")
